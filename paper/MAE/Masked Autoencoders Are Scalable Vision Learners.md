# 0. 总结

这篇论文提出了一个称为Masked Autoencoders (MAE) 的自监督视觉表示学习方法。论文的主要内容如下:

1. 提出了一种简单有效的自监督学习框架MAE。该方法通过遮蔽图像的随机patch,并训练一个自动编码器来重构丢失的patch。

2. MAE使用了一种非对称的编码器-解码器设计。编码器只对可见的patch进行操作,而轻量级的解码器则从编码器的表示和遮蔽token中重构完整图像。

3. 高比例的遮蔽(如75%)可以产生有挑战性的自监督任务,迫使模型进行更高层次的理解。同时也可以大大减少训练时间。

4. 解码器设计灵活,只需要很浅的网络即可。这样可以专注编码器表示的抽象语义级别。

5. 在ImageNet数据集上进行评估,MAE预训练可以获得很好的效果,特别是对大模型尺寸有明显改进。在多个下游任务中也显示出很强的迁移学习能力。

6. MAE重构像素而不是tokens,实现简单且高效。结果显示与token化重构相比毫无劣势,但更简单快速。

7. 与对比学习方法相比,MAE对数据增强不太敏感,主要依赖遮蔽产生新的训练样本。

8. MAE表现出可扩展性,与NLP中类似的自监督方法趋势一致,有望探索更大模型规模。

总体来说,本文通过探索视觉与语言信号的区别,设计出一个简单高效的MAE框架,在多个视觉任务上都取得了非常强劲的结果,展示了自监督学习在计算机视觉中可扩展的潜力。

**结构**:

1. 输入图像被分成patches,随机遮蔽其中一部分patches。

2. MAE编码器:只对可见patches进行操作的Transformer Encoder。

3. MAE解码器:另一个更浅的Transformer Decoder网络,输入为编码器输出和遮蔽的patches的Mask Tokens。

4. 解码器输出每个patches的重构像素。

**训练**:

1. 对输入图像进行patch分割和随机遮蔽。

2. 可见patches传入MAE编码器得到编码表示。

3. 将编码表示和Mask Tokens一起输入解码器。

4. 计算解码器输出与原图像之间的MSE重构损失,仅对遮蔽patch计算损失。

5. 通过back propagation更新编码器和解码器的参数。

6. 重复上述过程,在图像数据集上预训练模型。

7. 最后抛弃解码器,将编码器应用于下游视觉任务中。

MAE使用编码器-解码器和高比例遮蔽的设计,可以学习对全局语义的理解,并且训练高效。它展示了在计算机视觉中进行自监督预训练的可扩展性。

# 1. Introduction

1) 在自然语言处理(NLP)领域,基于遮蔽自动编码的自监督预训练方法(如BERT)已经非常成功,并且展现出很好的可扩展性。

2) 这种遮蔽自动编码的思想本质上也适用于计算机视觉,但迄今为止的进展落后于NLP。论文分析了图像和语言信号之间的一些关键差异:

(i) 模型架构差异已被Vision Transformer (ViT)解决。

(ii) 图像中的信息密度更低,需要更高比例的遮蔽来产生有意义的任务。

(iii) 在图像中,解码器的作用不同,其输出是像素级别,语义级别更低。

3) 针对这些差异,论文提出了一个简单有效的MAE框架,具有非对称编码器-解码器设计。

4) 使用高比例随机遮蔽可显著降低训练时间,同时创建一个需要更高层语义理解的任务。

5) 在ImageNet数据集上,MAE预训练可以获得非常强劲的效果,尤其是对大模型而言。

6) 希望这种简单的自监督学习框架能启发计算机视觉继续探索模型规模的扩展,如同NLP的发展轨迹。

总之,Introduction部分概述了论文的动机和主要方法、结果。

### 补充知识点

#### self-supervised pre-training

self-supervised pre-training指在机器学习中使用自监督学习方式进行预训练的过程。

其关键特点是:

1. 不需要人工标注的数据。仅使用原始非标注的数据进行预训练。

2. 通过设计各种预训练任务,让模型学习到对下游任务有用的表示。这些预训练任务不需要标签,通常是定义在输入数据本身上的。

3. 预训练完成后,通过微调(fine-tuning)的方式,将预训练的模型应用到实际的下游监督学习任务中。

4. 与随机初始化相比,自监督预训练可以获得更好的 generalization performance,并且对下游任务的样本复杂度要求更低。

自监督预训练广泛用于自然语言处理和计算机视觉。代表性方法包括BERT、GPT在NLP中,以及Contrastive Learning、Masked Autoencoders等在计算机视觉中。它们都展示了通过自监督预训练可以学习到非常强大的表示,从而提升各下游任务的性能。

总之,self-supervised pre-training是一种无需人工标注数据就可以学习通用表示的重要范式,正在被广泛应用于NLP和CV中。

#### autoregressive 和 masked autoencoding

autoregressive 和 masked autoencoding 是两个流行的自监督预训练范式。

autoregressive:

- 输入是一个序列,模型需要预测序列中被遮蔽的单元(例如词语)。

- 模型每次只能看到序列的部分内容,需要依赖前文内容来预测遮蔽位置。

- 代表方法是NLP中的GPT。

masked autoencoding:

- 输入是一个完整序列,模型需要重构被遮蔽的部分。

- 模型可以看到整个序列,但其中的部分被遮蔽。需要从非遮蔽内容中重构出遮蔽的内容。 

- 代表方法是NLP中的BERT。

两者主要区别:

- Autoregressive依赖序列前文,masked autoencoding利用整体上下文。

- Autoregressive是生成模型,masked autoencoding是重构模型。

- Autoregressive逐字生成,masked autoencoding直接输出遮蔽内容。

两类方法都展示了强大的预训练效果。Autoregressive更适合生成任务,而masked autoencoding被广泛认为是一种通用的预训练范式。

#### mask token 和 positional embedding

mask token 和 positional embedding 在自监督预训练模型中扮演重要角色:

1. mask token 表示被遮蔽的位置。在像BERT这样的遮蔽自动编码模型中,输入序列中的某些位置被遮蔽,这些位置使用特殊的mask token作为占位符。模型需要预测这些位置的原始内容。

2. positional embedding 为序列中的每个位置编码位置信息。因为标准的神经网络通常难以学习序列 order 的信息,需要显式地注入位置编码,来表示一个词在序列中所处的位置。

具体来说:

- mask token是一个学习得到的向量,被置换到序列被遮蔽的位置。模型需要预测这个位置的内容。

- positional embedding通常是基于正弦和余弦函数的固定编码,为每个位置编码一个向量。这些向量和输入Embedding相加。

- Transformer类模型广泛使用上述两者来表示序列信息。

综上,mask token和positional embedding在自监督预训练中充当关键的辅助输入,为模型提供输入遮蔽的位置和顺序信息,促进模型学习语义表示。它们现在被广泛使用。

#### Vision Transformers (ViT)

Vision Transformers (ViT) 是将Transformer模型应用于计算机视觉任务的一类模型。

其主要特点包括:

1. 将图像分割成固定大小的patch,每个patch通过线性投影得到一个向量表示。

2. 在patch的向量表示上添加positional embedding,表征每个patch的位置信息。

3. 将patch的向量序列作为输入,送入TransformerEncoder进行处理。Transformer Encoder由自注意力模块和MLP组成。

4. 通过Transformer Encoder输出获得图像的最终特征表示,然后用于下游视觉任务。

5. 与卷积网络不同,ViT完全基于attention机制,不使用卷积操作。

ViT第一次展示了Transformer模型在图像识别任务上也可以达到强劲的效果。一系列后续工作不断优化ViT,使其成为计算机视觉一个重要的模型系列。

ViT消除了卷积网络与Transformer在架构上的鸿沟,使得Transformer能够用于图像中,也为视觉的自监督表示学习打开了新的大门。因此它是一个重要的里程碑式的工作。

# 2. Related Work

论文的2. Related Work部分概述了以下与Masked Autoencoders (MAE)相关的研究工作:

1. 在自然语言处理领域,BERT和GPT等基于遮蔽语言模型的自监督预训练方法。它们启发了MAE在计算机视觉领域的探索。

2. 自动编码器是一类经典的表示学习方法。变体如Denoising Autoencoders探索了不同的输入损坏方式。MAE可以看作是其中的一种。

3. 基于遮蔽图像编码的方法,如Context Encoder使用卷积网络进行图像补全。近期的iGPT和BEiT等采用了Transformer。

4. 计算机视觉中其他类型的自监督表示学习方法,如基于图像上下文预测的方法、对比学习等。这些方法与MAE概念上有区别。 

5. Vision Transformer (ViT)启发了基于Transformer的MAE在视觉上的探索。

6. 一些细节思想,如遮蔽语言模型中使用的Mask Tokens和Positional Embeddings,也为MAE的设计提供了启发。

综上,Related Work部分概述了MAE技术上与之相关的研究背景和发展脉络。

# 3. Approach

论文的3. Approach部分详细介绍了Masked Autoencoder (MAE)的技术方法,包括以下内容:

### 3.1 Masking

将一个图像分割成规则的不重叠的patch。

按照均匀分布,随机遮蔽图像中的patch,遮蔽比例高达75%。

随机采样在很大程度上消除了冗余,防止了潜在的中心偏差,提供了高度稀疏的输入

### 3.2 MAE encoder

只对可见patch进行操作的Transformer Encoder。

只用一小部分的计算和内存来训练非常大的编码器。

### 3.3 MAE decoder

利用编码器输出和Mask Token进行解码,以重构原图像。

每个Mask Token是一个共享的、学习的向量，表明存在一个需要预测的遗漏patch。

向这个完整集中的所有标记添加positional embeddings

解码器仅在训练前用于执行图像重建任务,独立于编码器设计,设计灵活。

使用轻量级的解码器来处理，减少预训练时间。

### 3.4 Reconstruction target

只计算masked patches上的损失

以像素或归一化像素作为目标,进行MSE重构loss计算。

使用归一化像素作为重建目标，提高了表示质量。

### 3.5 Simple implementation

通过打乱和逆打乱Patch顺序来进行随机遮蔽采样,不需要特殊的稀疏操作。

**生成输入补丁的标记**：首先，通过线性投影和位置嵌入，为每个输入patch生成一个标记（token）。

**随机洗牌和遮罩**：接下来，对标记列表进行随机洗牌，并根据遮罩比例移除列表的最后一部分。这个过程会生成编码器的一个小子集标记，相当于无重复采样patch。

**编码器和遮罩标记**：在编码器之后，将遮罩标记列表附加到编码patch列表中，并对整个列表进行反洗牌（逆转随机洗牌操作），以使所有标记与其目标对齐。

**解码器**：解码器被应用于包含遮罩标记的完整标记列表（添加了位置嵌入）。

无需复杂数据增强,随机遮蔽本身产生新的训练样本。

探究了不同的遮蔽采样策略。

可以只训练部分Transformer块进行部分微调。

### 补充知识点

#### Transformer block

在 MAE 编码器中提到的 Transformer blocks 指的是 Transformer 模型中的基本组成模块。

Transformer 模型由编码器(Encoder)和解码器(Decoder)组成,编码器和解码器都包含多个相同的 Transformer blocks 堆叠而成。

每个 Transformer block 包含以下子模块:

1. Multi-head self-attention:实现对输入序列的自注意力操作,得到序列之间的相关性表示。

2. MLP: 多层全连接网络,在attention的基础上进一步处理表示。 

3. Layer normalization: 对中间结果进行规范化。

所以MAE编码器提到的Transformer blocks就是指组成Transformer编码器的这种基本模块的堆叠。

MAE解码器也使用了类似的Transformer blocks架构。

通过堆叠多个Transformer blocks,模型可以表示复杂的语义信息。这是Transformer模型的基本组成方式。

#### MLP

MLP(多层感知机)是一种简单而强大的前馈神经网络。

其特点是:

1. 包含输入层、输出层,以及一个或多个隐藏层。

2. 每层都全连接到下一层,不存在回环或反馈。

3. 每层之间通常使用非线性激活函数,如ReLU。

4. 可用作函数逼近器,通过训练将输入映射到输出。

5. 参数容量大,可以拟合复杂函数。

6. 计算流程简单,易于训练。

MLP广泛用于神经网络中,例如:

- 在卷积网络中用作分类头。

- 在Transformer中用作网络中间的非线性变换。

- 也可作为整个模型,如多层感知机分类器。

相比卷积层,MLP没有局部连接和权重共享的属性。但可逼近更广范围的函数。

MLP是一个概念简单但强大的网络模块,被广泛使用。与卷积层并用可利用两者优势。

# 4. ImageNet Experiments

### 4.1 Main Properties

论文4.1节阐述了在ImageNet上对MAE的核心特性进行的分析,主要包括:

1. 遮蔽率: 75%的高遮蔽率对微调和线性评估都 works well。这与BERT使用的15%遮蔽率形成对比。

2. 解码器设计:解码器可以很浅,但需要有一定深度以适应线性评估。解码器宽度也可以比编码器窄很多。

3. Mask Token: 在编码器中移除Mask Token可以获得更好的结果,同时也加速训练。

4. 重构目标:以像素为目标效果很好,tokenization 的改进不明显。

5. 数据增强:MAE即使没有复杂的数据增强也可以达到不错的效果。

6. 遮蔽采样:随机采样是最好的采样策略。不同策略会产生不同难度的自监督任务。

7. 训练进度:需要较长的训练进度才能充分收敛,达到更好的效果。

总之,通过大量的ablation实验对MAE的关键设计因素进行了分析,得到了对该方法行为的深入理解。

### 4.2 Comparisons with Previous Results

论文4.2节与之前工作进行了比较,内容包括:

1. 在ImageNet上,与其他自监督方法比较,MAE取得更好的fine-tuning效果,特别是对大模型而言。

2. MAE使用纯像素重构就超过了BEiT使用tokenizer的结果,而且MAE更简单高效。

3. 与有监督预训练相比,MAE显示出更好的可扩展性,随着模型变大效果提升更多,趋势上与JFT-300M预训练相似。

4. 在线性评估上,MAE仍落后于对比学习方法,但fine-tuning结果更强。这表明两类方法侧重点不同,线性可分性不是唯一评价标准。

5. MAE在图像稳健性数据集上的零训练迁移也展现出可扩展的趋势。

总结来说,这部分与SOTA量化比较了MAE在ImageNet上的效果质量,并讨论了一些关键洞见,如线性可分性的意义等。

### 4.3 Partial Fine-tuning

4.3节探讨了partial fine-tuning,即只微调模型的一部分层而固定其他层。其中发现:

1. 仅微调一层Transform Block,准确率就大幅提升,远超线性评估。这表明MAE学习的是强大的非线性特征。

2. 与对比学习方法MoCo v3相比,如果微调较多层,MAE始终优于其线性评估结果。这与fine-tuning结果一致,说明MAE学习的是更佳的非线性特征。

3. 微调少量层就可达到接近完全fine-tuning的效果。

4. 线性可分性不是唯一的表示质量评价指标。结合fine-tuning看到MAE学习到更强的非线性特征。

这部分讨论提供了新的洞察:线性评估和fine-tuning具有不同侧重点,线性可分性并不是所有的,需要综合评价。

### 补充知识点

#### end-to-end fine-tuning 和 linear probing

end-to-end fine-tuning 和 linear probing 是评估自监督预训练模型表示能力的两种常用范式:

1. end-to-end fine-tuning:
- 加载预训练模型作为初始化
- 在下游任务的训练集上微调整个模型
- 评估下游任务的性能
- 模型各层表示都被微调过,更好地适应下游

2. linear probing:
- 仅使用预训练模型的编码器作为固定的特征提取器
- 在其输出特征上训练一个线性分类器 
- 评估线性分类器在下游任务上的表现
- 更直接评估预训练模型编码器特征的可分离性

两者侧重点不同:
- Fine-tuning评估特征的适应性和微调的有效性
- Linear probing更纯粹地评估预训练特征的可分离性

研究发现,两者的结论不完全相关。综合使用两种评估可以更全面地理解预训练表示的质量。

# 5. Transfer Learning Experiments

论文的5. Transfer Learning Experiments部分报告了在下游任务上的迁移学习结果,主要有:

1. 在COCO对象检测和分割中,MAE预训练明显优于对应的有监督预训练,尤其是在大模型上改进更多。

2. 在ADE20K语义分割中,MAE同样显著优于有监督预训练基线。

3. 在多个分类数据集上,MAE都达到了state-of-the-art的结果。vg9-1-1

4. 使用像素和词袋重构的对比表明,词袋化没有明显优势,但会增加训练负担。

5. 总体来说,MAE展现了强大的迁移学习能力,并显示出可扩展性。vg7-7-7

综上,该部分验证了MAE学习到的表示对下游任务具有很强的泛化能力,特别是当模型尺寸增大时,提升更为明显。

### 补充知识点

#### Transfer Learning

Transfer Learning(迁移学习) 是机器学习中的一类技术,其核心思想是:

利用从一个任务或域中获得的知识,来帮助改进另一个相关但是不同的任务或域的学习。

其典型做法包括:

1. 初始化模型参数。使用预训练模型的参数或特征提取器初始化,而不是从零开始或者随机初始化。

2. 微调模型。在目标任务的数据集上微调预训练模型的参数,而不是全新训练。

3. 固定/冻结特征提取器。只训练新增的参数,保持预训练参数固定。

4. 多任务学习。联合训练源任务和目标任务获取共享表示。

Transfer Learning主要依赖源任务和目标任务之间存在的相关性或规律的相似性。它可以大大减少目标任务对数据量和计算的需求。

在计算机视觉与自然语言处理中都被广泛使用。 ImageNet预训练模型就是一个典型例子。

# 6. Discussion and Conclusion

1. 简单可扩展的算法是深度学习的核心。MAE作为一种简单的自监督学习算法,展示了与NLP类似的可扩展性。

2. 图像与语言有本质区别,需要仔细考虑信号的差异性。MAE使用高密度随机遮蔽来应对图像的冗余性。

3. MAE产生复杂的整体推理,这可能与学习有用表示相关。这种行为类似于语义理解。

4. 在ImageNet及下游任务中,MAE都显示出可扩展性,尤其是当模型变大时,这与NLP中类似。

5. 总结来说,MAE是一个简单、有效、可扩展的框架,为视觉的自监督表示学习打开了新的大门。

6. 需要考虑算法可能学习到的数据偏见,及其负面影响。

该部分总结了全文贡献,讨论了图像与语言的区别,并呼吁社会责任感。它呼应了摘要与引言,很好地总结了全文的核心思想。