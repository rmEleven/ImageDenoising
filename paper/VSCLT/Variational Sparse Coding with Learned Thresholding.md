# 0. 总结

这篇论文提出了一种新的变分稀疏编码方法,通过对来自推断网络的样本进行阈值处理来学习稀疏分布。

论文的主要内容包括:

1. 提出了一种新的变分稀疏编码方法,通过对高斯分布或拉普拉斯分布采样并进行平移软阈值处理,可以得到等价于Spike-and-Slab分布的稀疏样本。这避免了使用离散随机变量的近似。

2. 通过直通估计器进行训练,在正向传递时跳过阈值函数的梯度计算,避免了训练不稳定。

3. 提出了最大下界损失采样方法,鼓励重复使用已开发的潜在特征。

4. 在线性生成器实验中,与FISTA、高斯先验、拉普拉斯先验等方法进行比较,阈值拉普拉斯先验效果最好。

5. 在Fashion MNIST和CelebA数据集上用DNN生成器进行实验,与高斯先验和Spike-and-Slab先验进行比较,表明提出的方法统计效率更高。

6. 通过估计线性字典来可视化DNN生成器学习到的特征,稀疏先验学习到的特征更具可解释性。

总体来说,文章通过阈值处理的方式,提供了一种新的变分法进行稀疏编码的方法,并通过实验表明其统计效率更高,学习到的特征更可解释。

## 0.1 网络结构

(1) 推断网络:多层感知机或卷积神经网络,用于学习后验分布 $q(z|x)$ 的参数。

(2) 生成网络:可以是线性生成器,也可以是卷积神经网络或变分自编码器中的解码器。用于学习 $p(x|z)$。

## 0.2 工作流程

(1) 对每个输入 $x$ ,推断网络输出后验分布的参数,然后采样以获得稀疏的潜在代码 $z$。

(2) $z$ 通过生成网络进行解码,重构输入,计算重构损失 $log p(x|z)$。

(3) 计算变分后验与指定的稀疏先验分布(如拉普拉斯)之间的KL散度,作为正则项。

(4) 优化推断网络和生成网络的参数,以最大化变分Evidence Lower Bound (ELBO)。

(5) 在采样 $z$ 时,对原始样本进行阈值处理,以获得精确为零的特征。

(6) 在训练时,使用直通估计器避免阈值操作的不可导问题。

(7) 使用最大化ELBO采样策略,而不是取平均,以鼓励特征的重复使用。

(8) 重复上述过程,直到网络收敛,学习到可重复使用的稀疏特征z。

综上,该模型完整地利用了变分框架、阈值采样和最大化ELBO采样策略来进行高效、精确和可解释的稀疏编码。

## 0.3 稀疏和变分

这篇论文提出的变分稀疏自动编码器模型主要通过以下两点来实现稀疏性和变分推断:

1. 实现稀疏性

- 在采样时,对来自推断网络的连续分布(高斯或拉普拉斯)样本应用阈值操作。

- 证明阈值样本与精确稀疏的Spike-and-Slab分布等价。

- 通过调整阈值量,可以控制码的稀疏性,获得理想的稀疏特征。

- 避免了现有方法中使用释放技巧带来的近似误差。

2. 实现变分

- 使用神经网络来参数化变分后验分布q(z|x),进行黑盒变分推断。

- 最大化变分下界,实现对后验分布q(z|x)到真实后验p(z|x)的逼近。

- 推断网络学习特征提取,生成网络学习重构输入。

- 使用重参数技巧采样,直通估计器训练,实现高效的变分框架。

- 最大化ELBO采样策略引入智能的特征选择。

综上,该模型通过精确的阈值采样与变分网络框架的结合,实现了高效、精确和可解释的稀疏变分推断。

# 1. Introduction

这篇论文的Introduction部分主要介绍了以下内容:

1. 稀疏编码策略因利用数据的低维结构进行简洁表示而受到赞誉。但是,稀疏编码的推理通常依赖优化过程,在高维问题中计算扩展性差。

2. 例如,在深度神经网络的高维中间层进行稀疏推理,需要在每次训练步骤进行迭代最小化。

3. 为了进行低计算量的推理,最近的黑盒变分推断方法使用DNN来估计后验分布的参数。

4. 但是,这些方法要么不明确地学习稀疏特征,要么依赖于在训练期间导致糟糕的梯度估计的松弛。

5. 受软阈值在迭代优化过程中的成功的启发,作者提出了一种变分稀疏编码方法,通过对从拉普拉斯或高斯分布中抽取的样本进行阈值处理来学习稀疏分布。

6. 作者从分析上显示,来自拉普拉斯分布的阈值样本与Spike-and-Slab分布等价,通过调整阈值超参数可以控制稀疏程度。

7. 在稀疏程度事先未知的情况下,作者还提出了一种技术来学习阈值参数的分布。

8. 为了训练推理网络,作者应用了直通估计器,跳过阈值函数的梯度计算,导致了良好的训练稳定性和梯度估计。

9.  作者提出了一种新的采样程序，鼓励特性重用，鼓励生成器学习更多样化的特性。

总结一下,Introduction部分首先介绍了稀疏编码的重要性,指出当前方法在高维表示上的问题,然后提出文章的方法来应对这一问题,并概述了文章的主要贡献。

## 补充知识点

### 1. Laplacian distribution

Laplacian distribution(拉普拉斯分布)是一种连续概率分布。

它的概率密度函数为:

$f(x|μ,b) = \displaystyle\frac{1}{2b}e^{-\frac{|x - μ|}{b}}$

其中,

$μ$ - 分布的位置参数,表示分布的中心位置
$b$ - 分布的尺度参数,控制分布的宽度

拉普拉斯分布具有以下特点:

1. 分布峰值在位置参数 $μ$ 处。

2. 分布曲线关于位置参数 $μ$ 对称。

3. 当 $b$ 增大时,分布曲线变平缓;当 $b$ 减小时,分布曲线变陡峭。

4. 拉普拉斯分布的方差为 $2b^2$。

5. 拉普拉斯分布具有更高的峰值和更重的尾巴,与高斯分布相比更能表示实际数据中的突出值。

6. 拉普拉斯分布常被用来作为先验分布,进行贝叶斯估计和稀疏推理。

总之,拉普拉斯分布是一种单峰对称分布,由位置参数和尺度参数控制,具有高峰和重尾特点,常用于稀疏建模。

### 2. Laplacian or Gaussian distribution

在设计先验分布时,选择拉普拉斯分布和高斯分布主要基于以下考虑:

1. 数据的特点

如果数据存在突出值或重尾特点,拉普拉斯分布由于具有高峰和重尾更能模拟这种数据分布。如果数据较为对称,不存在明显的突出值,高斯分布会更为合适。

2. 模型的稀疏性

拉普拉斯分布有利于产生稀疏解,因为其峰值高而尾部衰减快。高斯分布没有这种明确的稀疏性偏好。如果需要获得稀疏特征或稀疏系数,拉普拉斯分布更有利。

3. 参数估计的难易程度

高斯分布的最大似然估计比较简单,而拉普拉斯分布的参数估计较难。如果需要简单快速地得到参数估计,高斯分布更合适。

4. 模型的鲁棒性

拉普拉斯分布对异常数据更为鲁棒,因为其尾部衰减较慢。如果需要模型对异常值更为鲁棒,拉普拉斯分布更佳。

5. 理论保证

在一些问题上,拉普拉斯分布先验可以使问题更易分析,得到理论保证,这时拉普拉斯分布更为合适。

6. 采样复杂度

从高斯分布采样更简单。如果需要大量采样,高斯分布在计算上更有效。

综合这些因素,就可以根据具体问题情况,在拉普拉斯分布和高斯分布中做出选择。一般来说,如果需要稀疏性和鲁棒性,拉普拉斯分布更合适;如果需要简单快速的参数估计和采样,高斯分布更为适用。

### 3. Spike-and-Slab distribution

[对Spike-and-Slab distribution的通俗理解](https://www.zhihu.com/question/51980224)

Spike-and-Slab分布是一种混合分布,用于产生稀疏解。其基本思想是:

1. 首先生成一个伯努利随机变量,决定这个维度是否取值为0(spike)。

2. 如果不为0,则从一个连续分布(slab distribution)中随机生成这个维度的取值。

Spike-and-Slab 分布的概率密度函数为:

$p(x) = γ δ(x) + (1-γ) f(x)$

其中:

$δ(x)$ - 在0处为1,否则为0的狄拉克 $δ$ 函数,表示spike

$γ$ - 取值为0的概率

$f(x)$ - slab分布,通常为正态分布或其他连续分布

Spike-and-Slab分布的主要特点是:

1. 可以明确地产生部分维度为0的稀疏样本。

2. 通过 $γ$ 控制稀疏度。$γ$ 越大,稀疏度越高。

3. slab分布 $f(x)$ 控制非零元素的分布。

4. 计算生成Spike-and-Slab样本较为简单。

总结来说,Spike-and-Slab分布通过混合一个点质量和一个连续分布,可以方便地生成稀疏样本,因此经常被用于变分稀疏推理中作为先验分布。它提供了一种明确且可控的方法来获得稀疏解。

相比于 L1 loss 等稀疏方法，spike and slab 给出的是一种更为精准、细粒度 的稀疏建模——我们对模型中的每一个参数，能逐个放置特有的稀疏先验：$\rho_0$，也能在推断后，得到刻画每一个参数被选择可能性大小的稀疏概率 $c(\rho_j)$。

### 4. straight-through estimator

straight-through estimator(直通估计器)是一种用于训练含有非平滑层(如符号函数、阈值函数等)的神经网络的技巧。

其基本思想是:

在前向传播时,直接传播非平滑层的输出。

在反向传播时,跳过非平滑层,直接传播梯度。

例如,对于符号函数y=sign(x),直通估计器按照以下方式运作:

前向传播:y = sign(x) 

反向传播: dy/dx = 1 (跳过sign函数的导数0)

直通估计器的优点是:

1. 可以方便地训练含有非平滑层的网络。

2. 避免了非平滑层对训练造成的困难。

3. 计算效率高,不需要求复杂的次梯度或次优化。

直通估计器的缺点是:

1. 破坏了反向传播的精确性,梯度方向可能不准确。

2. 网络训练可能会出现一定的偏差。

3. 需要较为仔细地设置学习率、初始化等超参数。

总结而言,直通估计器通过跳过非平滑层的反向传播,提供了一种简单高效的方式来训练含非平滑层的网络,但也有一定的缺点。需要权衡估计偏差和计算效率。

# 2. Related Work

## 2.1. MAP Estimate/Regressive Inference

1. 稀疏模型通常通过最大后验估计(MAP)来推断稀疏潜在特征,这需要求解一个迭代优化问题。

2. 尽管存在在离散和连续时间内解决这种优化问题的方法，但它们的计算成本通常与维度的关系很差，这使得它们无法在现代深度学习环境中使用。

3. 另一种推断方法是使用DNN来回归给定输入数据的稀疏码,但需要真实稀疏码进行监督训练。

4. 这种方法在无监督学习中受限,因为它需要稀疏码的真实标签。

总结而言,这部分论述了当前基于MAP估计的稀疏推理方法存在计算量大的问题,尤其不适合在高维 DNN 表示中使用。文献提出了一种无监督学习线性生成器的方法,但也需要优化迭代过程。

## 2.2. Variational Inference

论文第2.2节概述了变分推断在稀疏编码中的相关工作,主要内容有:

1. 早期的变分推断方法对指数族应用于稀疏编码,使用迭代过程来拟合变分后验分布。

2. 后来的工作探索了变分方法在Spike-and-Slab模型中的应用,使用近似分析解来进行更快的推断。

3. 但这些方法没有有效扩展到高维DNN表示。

4. BBVI的提出,使用DNN来估计变分后验分布的参数,极大提升了变分推断的计算效率。

5. 但是基于DNN的BBVI在不同的先验下存在一些问题:

    - Laplacian先验:所有特征均为非零
    
    - Spike-and-Slab先验:需要连续松弛近似离散随机变量
    
    - Beta-Bernoulli先验:样本不明确稀疏

6. 一些后续工作尝试通过分层模型、演化优化等方式引入稀疏性。

7. 但这些方法没有提供一种简单直接的方式来获得精确稀疏样本。

总结而言,这部分概述了传统变分方法在稀疏编码中的限制,以及基于DNN的BBVI存在的问题,指出当前方法没有有效获得稀疏解的简单方法。

## 2.3. Estimating the Variational Bound

## 补充知识点

### 1. variational bound

variational bound(变分下界)是变分推断中的一个重要概念。

在变分推断中,我们希望最大化数据的边缘对数似然 $log p(x)$。但是直接优化它是很难的。

所以我们引入一个变分后验分布 $q(z)$,然后有:

$log p(x) = L(q) + KL(q||p)$

其中,$L(q)$ 是ELBO(evidence lower bound),定义为:

$L(q) = E_q[log p(x|z)] - KL(q||p(z))$

KL项大于等于0,所以 $L(q)$ 是一个 $log p(x)$ 的下界。我们可以最大化 $L(q)$ 来优化变分分布q。

所以, $L(q)$ 就是变分下界,它以下界了我们最终想优化的目标 $log p(x)$。

变分下界的主要优点是:

1. $L(q)$ 可以更简单地优化,不需要计算 $log p(x)$。

2. 最大化 $L(q)$ 同时考虑了重构误差和两分布的相似度。

3. 提供了一种近似求解真后验 $p(z|x)$ 的方法。

总结而言,变分下界是变分推断的核心,它为我们提供了一种更简单优化真后验分布的方法,是变分推断可行的理论基础。

# 3. Methods

3.1 BBVI部分:

介绍了变分自动编码器框架,使用神经网络来参数化后验分布,通过最大化ELBO来进行变分推断。

3.2 阈值样本重参数化部分:

在BBVI的框架下,提出通过对采样进行阈值处理来获得精确的稀疏样本。证明了阈值样本与Spike-and-Slab分布等价。

3.3 Max ELBO采样部分:

在BBVI采样时,提出一种最大化ELBO的采样策略,鼓励特征重用。

联系:

(1) 3.2和3.3都是在3.1 BBVI框架下的改进。

(2) 3.2解决了BBVI无法获得精确稀疏性的问题。

(3) 3.3解决了BBVI中特征选择机制不足的问题。

(4) 3.2和3.3共同完善了BBVI框架,使其更适合稀疏编码。

(5) 3.2提供精确稀疏样本,3.3利用这一点引入特征选择机制。

综上,3.1建立框架,3.2提供精确稀疏采样,3.3优化采样策略,共同形成针对稀疏编码任务的可行和有效的变分推断方法。

## 3.1. Black-box Variational Inference

1. BBVI的核心思想是用神经网络来参数化变分分布 $q(z|x)$ ,以此逼近真实后验分布 $p(z|x)$。

2. 假设我们有数据 ${x_1, x_2,..., x_N}$ ,目标是最大化数据的边缘log似然 $∑log p(x_n)$。

3. 直接优化log似然是困难的,所以引入变分分布 $q(z|x)$ 构建变分下界:

$L(θ,φ;x) = E_q[log p(x,z)] - E_q[log q(z|x)]$

这里 $θ$ 是生成模型的参数, $φ$ 是变分分布的参数。

4. 可以证明最大化L相当于最小化KL散度 $KL(q||p)$,使q逼近p。

5. 使用EM算法优化L:

    E步:固定 $θ$,优化 $φ$ 使L最大,即使q逼近p
    
    M步:固定 $φ$,优化 $θ$ 使L最大,即提升生成模型

6. E步中,通过reparameterization trick采样来优化 $φ$ ,减小梯度方差。

7. M步中,通常通过SGD来优化 $θ$。

8. 重复E步和M步直到L收敛。

这样BBVI就通过神经网络参数化的q和迭代优化,使q逼近真后验p,从而进行可扩展的变分推断。

## 3.2. Reparameterization for Thresholded Samples

1. 标准BBVI采样时,所有特征均为非零,无法获得精确稀疏性。这是因为常见的高斯、拉普拉斯等先验分布采样不会得到特征完全等于零的样本。

2. 为了获得精确稀疏性,作者提出对来自推断网络的拉普拉斯或高斯分布样本,进行平移软阈值(shifted soft-thresholding)操作。

3. 推导证明,阈值后的样本与Spike-and-Slab分布等价。Spike-and-Slab分布可以明确地产生部分特征为零的稀疏样本。

4. 通过调整阈值量,可以控制样本的稀疏度,与Spike概率 $(1-γ)$ 呈对应关系。

5. 当阈值未知时,可以学习阈值参数的分布,采用随机伽马分布作为阈值的先验。

6. 训练时,为了避免阈值函数的不可导问题,使用直通估计器,跳过阈值函数的梯度计算,直接传递生成器的梯度。

7. 这样可以获得精确稀疏性,同时也保证了训练过程的稳定性和准确的梯度估计。

8. 详细阐述了重参数化采样和计算损失函数的过程。

重参数化采样过程:

(1) 推断网络输出基础分布(高斯或拉普拉斯)的参数 $μ,σ$ 或 $μ,b$。
(2) 从基础分布采样 $s~p(s)$ ,然后应用平移软阈值函数 $Tλ(s)$ 进行阈值处理。
(3) $Tλ(s)$ 的形式为: $sign(s-μ)max(|s-μ|-λ,0) + I[|s-μ|>λ]μ$。
(4) 这样得到阈值后的样本 $z=Tλ(s)$,分布与Spike-and-Slab等价。
    
损失函数计算:

(1) 生成器损失为 $log p(x|z)$,采样得到的 $z$ 输入生成器。
(2) KL散度损失针对基础分布 $p(s)$ 而不是阈值样本 $z$。
(3) 当学习阈值时,增添加阈值先验的KL散度项。
(4) 使用直通估计器训练时,生成器损失直接传播,跳过 $Tλ$ 的梯度。

推导表明,当 $|s-μ|≤λ$ 时, $z≡0$ 对应spike分布;当 $|s-μ|>λ$ 时, $z$ 对应slab分布。

控制 $λ$ 可以调整“尖峰”spike概率,实现不同稀疏度。

## 3.3. Max ELBO Sampling

论文3.3节提出了一种新的采样策略Max ELBO Sampling,主要内容如下:

1. 在变分推断中,需对后验分布q进行采样估计ELBO的期望。标准BBVI中是取多个独立样本的ELBO平均。

2. 但在稀疏先验下,不同样本会使用不同的特征支持(非零元素)。这会鼓励开发更多潜在特征。
   - 因为稀疏先验分布是多模式的,每个模式对应不同的特征组合
   - 对同一数据采样多个latent code,每个code使用的特征组合可能不同
   - 这会鼓励开发更多潜在特征,以适应不同的采样模式

3. 但是,MAP估计会优先选择最能代表数据的特征,这样可以最大程度减小损失。
   - MAP估计会选择使后验概率最大的点作为估计值
   - 后验概率由似然和先验组成,需要同时考虑重构loss和特征的概率
   - 对于稀疏先验,后验概率高的特征组合重构loss小,代表数据的能力强
   - 所以MAP会自动选择这种高后验概率的特征组合

4. 而BBVI中,先验项使每个特征被同等使用,没有这种自动的特征选择机制。
   - BBVI中采样是根据变分后验分布进行的
   - 先验项(KL散度)使每个特征被同等采样到
   - 没有像MAP那样自动选择最佳特征组合的机制

5. 为了引入这种机制,作者提出“最大化ELBO采样”策略。

6. 即从多个采样中,选择使ELBO最大的那一个样本,而不是求平均。

7. 这样可以鼓励重复使用已开发的特征,增加特征一致性。

8. 实验评估了不同采样方式下的一致性指标,证明了最大化ELBO采样可以提高一致性。

9.  即使这种采样提供了有偏的损失估计,但可以改进性能。

10. 文章给出了这种采样过程的详细算法。
    - 输入:训练数据,阈值参数,网络参数等
    - 进行J次采样,每次计算ELBO
    - 从J个ELBO中选择最大值
    - 更新网络参数,增大最大ELBO
    - 迭代进行采样、选择最大ELBO、更新网络

总之,这部分提出一种新的采样策略,通过最大化ELBO鼓励特征重用,可获得更好的性能。

# 4. Experiments

## 4.1. Linear Generator

### 4.1.1. SPARSE CODING PERFORMANCE

作者测试了不同的推断方法来进行图像块的稀疏编码

1. 实验数据为80000个16x16大小的图像块,用来进行稀疏编码。

2. 字典大小设置为256,也就是编码长度为256。

3. 作为基准,使用FISTA算法直接优化目标函数来得到代码,目标函数包含数据拟合、L1正则化和字典正则化三部分。

4. 变分方法中,训练推断网络,使ELBO最大化。使用高斯、拉普拉斯、Spike-and-Slab等不同先验分布。

5. Spike-and-Slab先验使用Gumbel-Softmax分布逼近伯努利分布,控制了稀疏度。

6. 对每个方法,分别用1个样本和20个样本进行训练和测试。

7. 结果显示,固定阈值的拉普拉斯先验效果最好,学习阈值时活跃特征数目增多,验证损失增加。

8. Spike-and-Slab先验使IWAE损失最小,但在稀疏编码任务效果不佳。

9. 计算了多信息量评估编码的统计效率。固定阈值的高斯先验多信息量最小。

10. 分析了生成器和推断网络的梯度SNR,阈值方法的梯度质量最好。

11. 展示了不同方法学到的字典,只有阈值拉普拉斯可以用1个样本学习到Gabor基。 

### 4.1.2. SAMPLING PERFORMANCE

1. 该实验目的是检测不同采样方法下,推断网络对同一输入数据的推断稳定性。

2. 度量标准为多个样本的支持集合的Jaccard相似系数。

3. 对固定输入数据,进行多次前向传播采样,计算每个样本的支持集合。

4. 计算所有样本支持集合两两间的Jaccard相似系数,然后取平均作为指标。

5. 这样可以评估同一输入在多次采样下,选择的特征是否一致。

6. 将max ELBO采样与平均ELBO采样进行比较。

7. 结果表明,max ELBO采样增加了特征选择的一致性。

8. 平均ELBO采样学习到了重复的字典元素,表示特征选择不一致。

9. max ELBO采样能学到更丰富且可重复使用的特征字典。

10. 因此该采样策略可以增加特征选择的稳定性,鼓励特征重用。

## 4.2. DNN Generator

1. 在CelebA和Fashion MNIST数据集上,将线性生成器替换为DNN生成器。

2. 保持编码长度不变,改变维度并固定稀疏度,探究不同先验的可拓展性。

3. 与高斯先验VAE比较,所有稀疏先验在增加维度时,多信息量、FID和MSE均有改进。

4. 随着稀疏度增大,阈值方法的MSE也较低。

5. 重构图像质量上,高维时稀疏先验也较好。

6. 学习编码后,使用它们来估计DNN生成器的等效线性字典。

7. 高斯先验的字典元素较杂乱,而稀疏先验学到的元素结构较清晰。

8. 使用CelebA的属性标签,分析每个特征与属性的相关性。

9. 稀疏先验学到的特征与属性语义相关性更强。

10. 结果再次验证,与高斯先验VAE相比,稀疏先验编码更具可解释性和可拓展性。

# 5. Discussion

1. 本文提出了一种新的变分稀疏编码方法,通过对样本阈值处理来学习稀疏分布。

2. 与当前方法相比,这种方法避免了使用松弛技术或离散随机变量的近似,能够获得精确的稀疏性。

3. 实验结果显示,在线性生成器和DNN生成器任务上,该方法取得了更好的性能和更高的统计效率。

4. 通过估计等效线性字典,发现该方法学到的特征更具可解释性。

5. 直通估计器避免了阈值操作的不可导问题,保证了稳定的训练。

6. Max ELBO采样增加了对有用特征的重复使用。

7. 今后的工作可以考虑替代KL散度的距离度量,以处理稀疏先验的非流形特性。

8. 总结该方法为稀疏编码提供了高效和可解释的变分推断解决方案。