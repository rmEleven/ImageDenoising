# 0. 总结

这篇论文的主要内容如下:

1. 提出了一种鉴别递归稀疏自动编码器(Discriminative Recurrent Sparse Auto-Encoder, DrSAE)模型,用于分类任务,比如手写数字识别。

2. DrSAE由递归的编码器组成,编码器使用ReLU激活函数,并展开成固定迭代次数。编码器连接到两个线性解码器,一个用于重构输入,另一个用于预测分类标签。

3. 模型首先通过无监督的方式训练,最小化稀疏重构误差。之后加入鉴别损失进行监督分类训练。

4. 递归结构允许不同层之间参数共享,具有像深层网络一样强大的表达能力,但参数更少。

5. 在MNIST数据集上实验显示,模型可以学习到层次化的表达,包含局部特征的“部分单元”和整合原型的“类别单元”。

6. 在加入鉴别损失后,类别单元才会出现,实现对不同类别的竞争。

7. 即使使用较少的参数,DrSAE也达到了很好的MNIST分类性能,优于其他技术。

8. 拆分实验验证了递归结构的重要性,并详细分析了学习到的表达形式。

总而言之,论文的创新点在于提出了一个结合鉴别目标的递归自动编码器,可以学习高效的层次化特征表达,在数字识别任务上取得了很好的效果。

# 1. Introduction

1. 深度网络可以利用数据的层次结构,通过多步运算逐步构建决策边界或输入变换。

2. 递归网络可以看作是深度网络的一种等价形式,其中不同层对应不同时间步。

3. 与独立层的参数相比,递归结构可以实现参数共享。

4. 稀疏编码也可以引入到递归网络中,形成强大的表示。

5. 本文提出的鉴别递归稀疏自动编码器(DrSAE)将递归、稀疏编码和鉴别训练结合。

6. DrSAE在时间维度展开后等价于一个深度网络,但参数被共享。

7. 时间展开提供了更强的表达能力,同时保持参数量不变。

8. 在MNIST数据集上验证,DrSAE可以学习出非常有效的表达。

### 补充知识点

#### backpropagation-through-time(BPTT)

backpropagation-through-time(BPTT)是训练递归神经网络的一种方法,是标准的backpropagation算法的扩展。

标准BP只针对静态的前向网络,而RNN中网络结构在时间上是展开的。BPTT的思想是:

1. 将循环网络在时间轴上展开成一个深层前馈网络。

2. 把时间步t看成网络的一层。

3. 通过时间展开的网络,运行正常的反向传播算法(backpropagation)。

4. 梯度会流经时间维度,所以可以更新动态循环部分的权重。

也就是说,BPTT沿时间维度反向传播误差,以此来训练循环网络。它等效于在时间轴上“展开”网络,然后进行标准反向传播。

这使得循环网络也可以像前馈网络一样进行有效的训练。BPTT是目前训练RNN的主要方法之一。

本文使用BPTT来训练鉴别递归稀疏自动编码器(DrSAE)模型。

#### Iterative Shrinkage and Threshold Algorithm(ISTA)

1. 优化问题
ISTA算法用于求解带L1规范化的优化问题:

$min_x ||x||_1 \ \ s.t.\ \ y = Ax$

这里$||x||_1$表示向量x的L1范数,用来限制x的稀疏性。$y = Ax$表示线性约束条件。

2. 迭代更新
ISTA通过迭代优化逼近最优解。其更新公式为:

$x_{k+1} = SoftThreshold(x_k - τA^T(Ax_k - y))$

这里$τ$是步长参数,$A^T$是矩阵$A$的转置。SoftThreshold表示软阈值化,使结果趋于稀疏。

3. 两步迭代
可以看到,ISTA分为两个步骤:

(1) 梯度步:$x_k - τA^T(Ax_k - y)$,沿梯度方向更新x,以减小数据项$||Ax-y||$

(2) 软阈值化:将x中的小值缩减为0,增强稀疏性

4. 收敛性
可以证明,如果步长$τ$设置合适,ISTA迭代可以收敛到最优解。

5. 解释
ISTA提供了一个功能可解释的框架,分别更新数据项和正则项,每步都可解释,这是它的优点。

本文模仿ISTA的迭代过程设计了模型结构,但使用了更加灵活的参数化形式。

#### 编码矩阵E和解释矩阵S

在迭代收缩阈值算法(ISTA)中使用的编码矩阵E和解释矩阵S表示如下:

1. 编码矩阵E
编码矩阵E等价于ISTA算法中系数矩阵$A$的转置,即$E = A^T$。

它将输入x映射到隐层特征空间,相当于特征编码器。

2. 解释矩阵S
解释矩阵S等价于ISTA中$I - τA^T * A$,即$S = I - τA^T A$。

其中$I$是单位矩阵,$τ$是步长参数。

S表示隐层之间的相关性,用于执行解释Away操作,消除冗余的特征。

3. ISTA限制
在标准ISTA中,E和S之间有严格的关系:$E = τS + I$。这限制了ISTA的适用性。

4. LISTA改进
LISTA改进之处在于,允许E和S为不相关的可学习矩阵,提升了表达能力和适用性。

5. DrSAE设计
本文的DrSAE模型采用了类似LISTA的灵活设计,E和S都是可学习的,不强制约束关系。

#### Learned Iterative Shrinkage and Threshold Algorithm(LISTA)

LISTA(Learned ISTA)是指学习型的ISTA算法,它是在ISTA基础上进行了改进。LISTA的相关说明如下:

1. 改进ISTA
LISTA的全称是Learned ISTA。它的思想是学习来逼近ISTA算法的迭代过程,而不是直接使用ISTA的更新公式。

2. 更灵活的参数化
具体来说,LISTA使用更加灵活的参数化形式来逼近ISTA中使用的两个矩阵:编码矩阵E和解释矩阵S。

3. 端到端训练
LISTA的所有参数E、S等都可以端到端训练,而不是像ISTA一样预先固定。

4. 更快的推理
因为使用了学习的方式逼近,LISTA可以使用很少的迭代次数来达到与ISTA相近的效果,因此推理更加高效。

5. 强化泛化能力
通过学习,LISTA可以从数据中自动获取相似迭代过程的知识,从而具有更强的泛化能力。

6. 启发本文模型
LISTA启发了本文提出的DrSAE模型,其也使用学习的编码矩阵E和解释矩阵S来逼近迭代优化过程。这种思想提升了模型的适用性。

#### 一个结论

论文中提到**ISTA计算的稀疏表示可以用结构类似但参数化更灵活的编码器进行有效逼近**的原因如下:

1. ISTA的参数限制
标准的ISTA算法中,编码矩阵E和解释矩阵S之间有固定的关系($E=τS+I$),这限制了表示能力。

2. LISTA的改进
LISTA通过学习的方式,使用更灵活的参数化形式来逼近ISTA的迭代过程,提升了适用面。

3. 结构类似
LISTA保留了ISTA的基本迭代结构,包含编码和解释两步骤,这保证了结构类似。

4. 参数更灵活 
但LISTA取消了E和S的严格关系,允许它们在端到端训练中自由学习,参数化更灵活。

5. 效果近似
尽管参数更灵活,但LISTA学习到的编码器可以有效逼近ISTA的稀疏编码结果。

6. 计算更高效
而且LISTA只需要很少的迭代次数就可以取得类似效果,计算更加高效。

7. 总结
通过更灵活的学习参数化,LISTA使用结构类似的编码器,以更高效的方式逼近了ISTA稀疏编码的结果。

# 2. Network architecture

## 网络结构

1. 编码器结构
- 递归编码器由Rectified Linear Units(ReLUs)组成
- 为递归结构,但按时间展开成多个步骤
- 输入x投影到每一个时间步(共T步)
- 每步的表示为n维向量zt,初始化为0

2. 连接解码器
- 编码器连接到两个线性解码器
- 一个m维解码器D用于重构输入
- 一个l维解码器C用于分类

3. 参数
- E是n×m的编码矩阵,S是n×n的解释矩阵
- b是n维的偏置向量
- 这些参数在每层时间步中共享

4. 训练损失 
- 先用无监督损失进行预训练
- 然后加入鉴别损失微调

5. 深度网络视角
- 展开的DrSAE可看作参数共享的深度网络
- 提高表达能力但参数不增加

6. 与ISTA的关系
- 满足一定约束时,DrSAE实现非负ISTA
- 但使用更灵活 parameterization,不受原ISTA限制

总之,该部分详细描绘了DrSAE的网络表达形式,解释了编码器、解码器等组成,以及与其他网络的关系。

## 计算公式

1. 编码器计算流程

$z^{t+1} = max(0, E·x + S·z^t - b) $

- zt+1是第t+1步编码器的输出
- E是编码矩阵,x是输入
- S是解释矩阵,zt是当前编码器输出  
- b为偏置向量
- max(0,·)实现ReLU非线性激活

2. 无监督损失函数

$L^U = ||x - D·z^T||^2_2 + λ·||z^T||_1$

- D是解码矩阵,zT是最后时间步编码器输出
- 第一项计算重构损失
- 第二项是L1正则化获得稀疏特征

3. 鉴别损失函数 

$L^S = logistic_y(C·\displaystyle\frac{z^T}{||z^T||})$

- C是分类矩阵,y是标签
- logistic计算多类logistic loss
- ||zT||进行L2归一化

## 训练流程

1. 初始化网络参数:编码矩阵E,解释矩阵S,偏置b,解码矩阵D和分类矩阵C等参数进行初始化。

2. 输入训练数据:将训练样本(x,y)输入网络,其中x是输入数据,y是标签。

3. 前向传播:通过递归编码器得到特征表示zT,然后通过解码器进行重构x̄和分类ŷ。

4. 计算损失:输出与标签之间的重构损失||x-x̄||2和分类损失logistic(ŷ,y)。

5. 反向传播:反向传播计算每个参数矩阵对损失函数的梯度。

6. 参数更新:采用梯度下降法更新各参数,降低损失函数。

7. 重复2-6:用遍历训练集中的所有样本,反复进行网络的前向传播、反向传播、参数更新。

8. 判别训练:首先进行无监督的重构预训练,获得较好的初始化参数。然后加入分类损失进行判别训练。

9. 分类预测:在测试阶段,丢弃解码器,只保留递归编码器进行特征提取,然后预测分类结果。

# 3. Analysis of the hidden unit representation

## 3.1 Part-units

1. 部分单元的编码器和解码器之间的角度较小,接近ISTA算法。

2. 从部分单元到部分单元,以及类别单元到部分单元的连接与ISTA理论预测值高度相关。

3. 这说明部分单元遵循ISTA规律,执行稀疏编码计算。

4. 部分单元将类别单元看作是参与重构的,并仅对类别单元未解释的残差输入进行稀疏编码。

5. 部分单元接收的连接来自编码向量与其对齐或者反向的单元。

6. 部分单元的解码向量具有局部和笔画特征。

7. 编码器与解码器的对齐程度决定了单元是否遵循ISTA算法。

### 补充知识点

#### angle between the encoder and decoder

论文中提到的编码器E和解码器D之间的“角度”,指的是向量间的夹角,具体表示的是:

1. 编码器E
编码器E是一个矩阵,其中每一行Ei表示输入x到第i个隐层单元的编码权重向量。

2. 解码器D
解码器D也是一个矩阵,其中每一列Di表示第i个隐层单元到重构输入x̂的解码权重向量。 

3. 夹角
可以计算Ei和Di这两个向量之间的夹角θ:

$θ = arccos( \displaystyle\frac{Ei·Di}{||Ei||·||Di||})$

这里·表示向量内积,||·||表示欧几里得范数。

1. 含义
这个夹角θ反映了第i个隐层单元的编码权重向量Ei和解码权重向量Di的方向对齐程度。

1. 分析
论文利用这个夹角θ来分析隐层单元学习到的是类别原型特征还是部分变形特征。

夹角θ小表示Ei和Di对齐,θ大则表示不对齐。

#### 判别类别单元和部分单元

编码器E和解码器D之间的"角度"可以用来分析隐层单元是类别原型特征还是部分变形特征,主要原因有:

1. 角度小表示encoding和decoding对齐

当编码器E的行向量和解码器D的列向量之间的角度很小时,表示它们高度对齐。

这通常意味着这个隐层单元学习到的是整体的原型特征。

2. 角度大表示encoding和decoding不对齐

相反,如果角度很大,则表示编码向量和解码向量不对齐。

这通常意味着这个隐层单元学习到的是原型的局部变形特征。

3. 类别原型的角度小

实验中可以观察到,表示类别原型的隐层单元其角度很小。

这说明其编码和解码端都是整体原型特征。

4. 部分变形的角度大

而表示部分变形的隐层单元其角度较大,编码和解码端不对齐。

5. 量化区分两类单元

因此,通过设定角度阈值,可以量化地区分这两类不同语义的隐层单元。

综上,角度大小反映了单元学习到的是整体原型还是局部变形,可以用来判别其类别特征或部分特征的属性。

## 3.2 Categorical-units

1. 类别单元的连接不遵循ISTA算法的预测。

2. 不同类别的部分单元与固定类别的类别单元相匹配。

3. 部分单元与类别单元原型平行则激活,正交则抑制。

4. 类别单元实现了高级池化,整合大量相关的部分单元。

5. 类别单元之间具有较强的抑制连接,实现winner-take-all机制。

6. 类别单元对重建贡献更大,由部分单元解释残差。 

7. 类别单元的活动更依赖于其他单元,也更晚出现。

8. 类别单元的解码更全局,类似原型。

# 4. Performance

1. 在MNIST手写数字数据集上测试DrSAE模型的分类性能。

2. DrSAE取得了1.08%的分类错误率,使用400个隐层单元。

3. 与其他递归网络和稀疏编码方法相比,DrSAE获得了最低的错误率。

4. 分析了递归结构的重要性,减少递归步数将导致错误率提高。

5. DrSAE获得好结果的原因在于学习到了层次化的表达形式。

6. 类别单元通过对部分单元的积聚实现类别原型表达。

7. 部分单元对残差进行编码,纠正类别单元的错误激活。

8. DrSAE完整地融合了递归、稀疏表达和鉴别训练的效果。

9. 实验证明DrSAE是一个高效可解释的模型,能够有效解决图像分类问题。

# 5. Discussion

1. DrSAE学习到的表达形式与数据分布的流形结构相关。

2. 类别单元表示流形上的点,部分单元表示切向量。

3. 输入被分解为类别原型和原型上的流形变换。

4. 流形变换保留了类别信息。

5. 类别单元通过积聚和竞争实现类别判断。

6. 部分单元解释类别单元激活模式。

7. 部分单元直接响应输入,类别单元需要积聚。

8. DrSAE学习原型-变形表达,而非独立部分表达。

9. 原型-变形表达更适合判别类别信息。

10. DrSAE成功建模了数据的流形分布结构。

11. 学习到的表达形式解释了DrSAE的卓越分类性能。